---
layout: post
title: 贝叶斯决策理论
key: 2018011102
tags: MachineLearning
mathjax: true
modify_date: 2018-01-12 20：42
---

>转载请标明出处：  
>https://seektech.github.io/2018/01/11/贝叶斯决策理论.html [**Miao LI (seektech)**](https://seektech.github.io/2018/01/11/贝叶斯决策理论.html)

贝叶斯决策理论研究了模式类的的概率结构完全知道的情况。

贝叶斯决策理论是解决模式分类问题的一种基本统计途径，假设了决策问题可以用概率的形式来描述并且假设所有有关的概率结构均已知。

条件概率公式：

$$\displaystyle p(x|y) = \frac {p(xy)} {p(y)} $$

贝叶斯公式：

$$\displaystyle p(B|A)=\frac {p(A|B)p(B)}{p(A)}$$

全概率公式：

$$p(A)=\sum \limits_{i=1}^n p(B_i)p(A|B_i)$$

**问题表示：**

类别-$\displaystyle \omega_i, i=1,…,c$

特征矢量-$\mathbf{x}=[x_1,\dots,x_d] \in \mathbb{R}^d$

先验概率-$p(\omega_i)$  $\sum \limits_{i=1}^c p(\omega_i)=1$

概率密度函数(条件概率)-$p(\mathbf{x} \mid \omega_i)$

后验概率-$\displaystyle p(\omega_i \mid \mathbf{x})=\frac {p(\mathbf{x} \mid \omega_i)p(\omega_i)}{p(\mathbf{x})}=\frac {p(\mathbf{x} \mid \omega_i)p(\omega_i)}{\sum \limits_{j=1}^c p(\mathbf{x} \mid \omega_j)p(\omega_j)}$

### [](#header-1)一、最小错误率决策

> 决策过程中自然要寻找使得决策错误率最小的决策行为

**1.** 当只知道先验概率

$$\displaystyle p(error) = \left \lbrace \begin{array} {c} p(\omega_2) \qquad if \ decide \ \omega_1 \\ p(\omega_1) \qquad if \ decide \ \omega_2 \end{array} \right.$$

最小错误率决策，即如果$p(\omega_1) >p(\omega_2)$，决策$\omega_1$，否则$\omega_2$

**2.** 基于后验概率决策

$$\displaystyle p(error) = \left \lbrace \begin{array} {c} p(\omega_2 | \mathbf{x}) \qquad if \ decide \ \omega_1 \\ p(\omega_1| \mathbf{x}) \qquad if \ decide \ \omega_2 \end{array} \right.$$

最小错误率决策，即最大后验概率决策，如果$p(\omega_1 \mid \mathbf{x}) >p(\omega_2 \mid \mathbf{x})$，决策$\omega_1$，否则$\omega_2$

根据后验搞率公式，如果$p(\mathbf{x} \mid \omega_1)p(\omega_1) >p(\mathbf{x}\mid \omega_1)p(\omega_2)$，决策$\omega_1$，否则$\omega_2$


### [](#header-2)二、最小风险决策（贝叶斯决策）

**1.** 最小风险决策

决策代价：将正确的类别$\omega_j$决策为$\alpha_i$时代价(loss)为 $\lambda_{ij}=\lambda(\alpha_i \mid \omega_j)$

条件风险(Conditional Risk)：$\displaystyle \mathrm{R}(\alpha_i \mid \mathbf{x})=\sum \limits_{j=1}^c \lambda(\alpha_i \mid \omega_j)p(\omega_j \mid \mathbf{x}) $

期望风险：$\displaystyle \mathrm{R}=\int \nolimits \mathrm{R}(\alpha(\mathbf{x})\mid \mathbf{x}) p(\mathbf{x})d \mathbf{x}$

贝叶斯决策：$\displaystyle \arg \underset{i}{\min} \mathrm{R}(\alpha_i \mid \mathrm{x})$

**2-class case：**

$$\displaystyle \mathrm{R}(\alpha_1 \mid \mathbf{x}) = \lambda_{11} p(\omega_1 \mid \mathbf{x})+\lambda_{12} p(\omega_2 \mid \mathbf{x})$$

$$\displaystyle \mathrm{R}(\alpha_2 \mid \mathbf{x}) = \lambda_{21} p(\omega_1 \mid \mathbf{x})+\lambda_{22} p(\omega_2 \mid \mathbf{x})$$

决策$\omega_1$，如果$\mathrm{R}(\alpha_1 \mid \mathbf{x}) <\mathrm{R}(\alpha_2 \mid \mathbf{x})$，否则$\omega_1$

即，

$$\displaystyle \begin{align*} \lambda_{11} p(\omega_1 \mid \mathbf{x})+\lambda_{12} p(\omega_2 \mid \mathbf{x}) &< \lambda_{21} p(\omega_1 \mid \mathbf{x})+\lambda_{22} p(\omega_2 \mid \mathbf{x}) \\ \displaystyle (\lambda_{21}-\lambda_{11})p(\mathbf{x} | \omega_1)p(\omega_1) & >(\lambda_{12}-\lambda_{22})p(\mathbf{x} | \omega_2)p(\omega_2)\\ \displaystyle \frac {p(\mathbf{x} | \omega_1)}{p(\mathbf{x} | \omega_2)} &> \frac {(\lambda_{12}-\lambda_{22})p(\omega_2)}{(\lambda_{21}-\lambda_{11})p(\omega_1) } \end{align*} $$

当决策代价为01损失时，

$$\displaystyle \lambda(\alpha_i \mid \omega_j) = \left \lbrace \begin{array}{c} 0 \qquad  i=j \\ 1 \qquad i \neq j\end{array} \right. \qquad i,j = 1, \dots ,c$$

$$\displaystyle \begin{align*} \mathrm{R}(\alpha_i \mid \mathbf{x}) & =\sum \limits_{j=1}^c \lambda(\alpha_i \mid \omega_j)p(\omega_j \mid \mathbf{x}) \\ & = \sum \limits_{j \neq i}p(\omega_j | \mathbf{x}) \\ & = 1-p(\omega_i | \mathbf{x})\end{align*}$$

最小风险决策，决策$\omega_i$，如果$p(\omega_i | \mathbf{x}) > p(\omega_j | \mathbf{x})$，这时的最小风险决策就等价于最大后验概率决策

**2.**带拒识的决策



**3.** 贝叶斯决策用于模式分类

分类过程：

- 计算样本属于每一类的后验概率
- 最大后验概率/最小风险决策

分类器设计：

- 收集训练样本
- 用每一类的样本估计类条件概率密度$p(\mathbf{x} \mid \omega_i)$
- 估计类先验概率


### [](#header-3)三、判别函数与决策面

**1.** 多类情况下的判别函数

有很多种 方式来表述模式分类器，使用最多的是判别函数$g_i(\mathbf{x}), i=1, \dots ,c$，如果对于所有的$j \neq i$，有$g_i(\mathbf{x}>g_j(\mathbf{x}))$，则分类器将此特征向量$\mathbf{x}$判决为$\omega_i$

与最小条件风险对应：

$$g_i(\mathbf{x})=-\mathrm{R}(\alpha_i | \mathbf{x})$$

与最大后验概率对应：

$$g_i(\mathbf{x})=p(\omega_i | \mathbf{x}) \ or \ g_i(\mathbf{x})=p(\mathbf{x} | \omega_i)p(\omega_i)$$

取对数便于计算和分析：

$$g_i(\mathbf{x})=\ln p(\mathbf{x} | \omega_i) + \ln p(\omega_i)$$

**2.** 两类情况下的判别函数

二类分类器，并非使用两个判别函数

定义一个简单的判别函数$g(\mathbf{x}) = g_1(\mathbf{x})-g_2(\mathbf{x})$，如果$g(\mathbf{x})>0$，则判为$\omega_1$，否则判为$\omega_2$

$$g(\mathbf{x})=p(\omega_1 | \mathbf{x})-p(\omega_2 | \mathbf{x})$$

$$\displaystyle g(\mathbf{x})=\ln \frac {p(\mathbf{x} | \omega_1)}{p(\mathbf{x} | \omega_2)} + \ln \frac {p(\omega_1)}{p(\omega_2)}$$

### [](#header-4)四、高斯密度函数

**1.** 单变量密度函数 $p(x) \sim \mathcal{N}(\mu , \sigma^2)$

$$\displaystyle p(x) = \frac {1}{\sqrt{2 \pi} \sigma} \exp \left [ -\frac{1}{2} \left( \frac{x-\mu}{\sigma}\right)^2\right]$$

$x$的期望值：

$$\displaystyle \mu \equiv \mathcal{E} [x] = \int_{-\infty}^\infty xp(x)dx$$

$x$的方差：

$$\displaystyle \sigma^2 = \mathcal{E}[(x-\mu)^2] = \int_{- \infty}^\infty(x-\mu)^2p(x)dx$$

![](https://ws3.sinaimg.cn/large/006tNc79ly1fnfy0ne7lxj30b107v74x.jpg)

单变量正太分布完全由两个参数决定，$p(x) \sim \mathcal{N}(\mu , \sigma^2)$，服从正态分布的样本聚集于均值附近，其散布程度与标准差有关，单变量正态分布大约有95%的区域在$\mid x-\mu \mid \leq 2\sigma$范围内，峰值为$p(\mu) = 1 / \sqrt{2 \pi} \sigma$

可以证明正态分布在所有具有给定的均值和方差的分布中具有最大熵：

（证明）

根据中心极限定理，大量独立随机变量之和趋于正态分布，实际环境中很多类别的特征分布趋于正态分布

**2.** 多元密度函数 $p(\mathbf{x}) \sim \mathcal{N}(\pmb\mu , \pmb\Sigma)$

一般的$d$维多元正态密度的形式如下，其中$\mathbf{x}$是一个$d$维列向量，$\pmb \mu$是$d$维均值向量，$\pmb \Sigma$	是$d \times d$的协方差矩阵，$\mid \pmb \Sigma \mid$和$\pmb \Sigma^{-1}$分别是协方差矩阵的行列式值和逆

$$\displaystyle p(\mathbf{x}) = \frac {1}{(2\pi)^{d/2}|\pmb \Sigma|^{1/2}} \exp \left[ -\frac{1}{2}(\mathbf{x}-\pmb \mu)^t \pmb \Sigma ^{-1}(\mathbf{x}-\pmb \mu)\right]$$

$$\displaystyle \pmb\mu \equiv \mathcal{E}[\mathbf{x}] = \int \mathbf{x}p(\mathbf{x})d\mathbf{x}$$

$$\displaystyle \pmb\Sigma \equiv \mathcal{E}[(\mathbf{x}-\pmb\mu)(\mathbf{x}-\pmb\mu)^t]=\int(\mathbf{x}-\pmb\mu)(\mathbf{x}-\pmb\mu)^tp(\mathbf{x})d\mathbf{x}$$

其中，如果$x_i$是$\mathbf{x}$的第$i$个元素，$\mu_i$是$\pmb \mu$的第$i$个元素，$\sigma_{ij}$是$\pmb \Sigma$的第$ij$个元素($x_i$和$x_j$统计独立，则$\sigma_{ij}=0$)，$\pmb \Sigma$的对角元素$\sigma_{ii}$是相应的$x_i$的方差，非对角元素$\sigma_{ij}$是相应的$x_i$和$x_j$的协方差

$$\mu_i = \mathcal{E}[x_i]$$

$$\sigma_{ij} = \mathcal{E}[(x_i-\mu_i)(x_j-\mu_j)]$$

**3.** 协方差矩阵$\pmb \Sigma$的性质

协方差矩阵的Eigenvalues & eigenvecters：

$$\pmb \Sigma \phi_i = \lambda_i \phi_i$$(特征值定义)

$$\pmb \Phi = [\phi_1\phi_2 \cdots\phi_d]$$

$$\pmb \Lambda = diag[\lambda_1\lambda_2\cdots\lambda_d]$$

$\pmb \Phi$为正交阵，$\pmb \Phi^t \pmb \Phi = \pmb I$，$\pmb \Phi^{-1}=\pmb \Phi^t$，

$$\Phi_i^t\Phi_j =\left \lbrace \begin{array}{c}  1, \ i=j\\ 0, \ i \neq j \end{array}\right.$$

矩阵表示：

$$\pmb \Sigma \pmb \Phi = \pmb\Phi \pmb \Lambda$$

$$\pmb \Sigma  = \pmb\Phi \pmb \Lambda\pmb \Phi^T= \sum \limits_{i=1}^d \lambda_i \Phi_i \Phi_i^T$$

矩阵对角化：

$$\pmb\Phi^T \pmb \Sigma\pmb \Phi = \Lambda$$

$$\pmb\Phi^T_i \pmb \Sigma \pmb \Phi_i = \lambda_i$$

应用-PCA(Principal Component Analysis )：

（PCA的推导）



**4.** 线性变换的高斯分布

服从正态分布的随机变量的线性组合，不管这些随机变量是独立的还是非独立的，也是一个正态分布

如果$p(\mathbf{x}) \sim \mathcal{N}(\pmb\mu , \pmb\Sigma)$，$\mathbf{A}$是一个$d \times k$的矩阵，$\mathbf{y}=\mathbf{A}^t\mathbf{x}$是一个$k$维向量，那么$p(\mathbf{y}) \sim \mathcal{N}(\mathbf{A}^t\pmb\mu , \mathbf{A}^t\pmb\Sigma \mathbf{A})$

在$k=1$且$\mathbf{A}$是一单位向量$\mathbf{a}$的情况下，$y=\mathbf{A}^t\mathbf{x}$是一标量，表示$\mathbf{x}$到$\mathbf{a}$方向的一条直线的投影，$\mathbf{a}^t \pmb\Sigma \mathbf{a}$是$\mathbf{x}$向$\mathbf{a}$投影的方差，产生了沿该直线方向的$\mathcal{N}(\mu , \sigma^2)$，因此可以计算数据验任何方向或自空间的分散程度

![](https://ws4.sinaimg.cn/large/006tNc79ly1fng74lfqz3j30b10b1di0.jpg)

如果定义矩阵$\pmb \Phi$，列向量是$\pmb \Sigma$的正交本征向量，$\pmb \Lambda$为相应本征值对应的对角矩阵，那么变换$\mathbf{A}_\omega=\pmb \Phi \pmb \Lambda^{-1/2}$将使变换后的分布的协方差矩阵成为单位阵，

$$\mathbf{A}_\omega^t \pmb \Sigma \mathbf{A}_\omega = \pmb \Lambda^{-1/2}\pmb \Phi^t \pmb \Sigma\pmb \Phi\pmb \Lambda^{-1/2} = \pmb \Lambda^{-1/2}\pmb \Lambda\pmb \Lambda^{-1/2} = \mathbf{I}$$

变换后的分布的本征向量谱具有均匀性，因此$A_\omega$被称为白化变换ZCA(Zero Component Analysis)

**5.** 白化变换ZCA推导



**6.** 相关知识

- Dimensionality reduction降维


- Feature extraction特征提取
  - Feature generation: original data—>$\mathbf{x}$
  - Linear feature extraction,$\mathbf{x} = \mathbf{A}^T\mathbf{d}$
- Feature selection (for reduction and performance)特征选择
  - Feature subset selection: a learning/optimization problem
- Feature transform (for extraction or reduction)特征变换
  - Linear transform, $\mathbf{y} = \mathbf{A}^T\mathbf{x}$
  - Nonlinear transform: may increase dimensionality, e.g. kernel PCA,kernel LDA
- Handcrafted feature手工特征
- Feature learning特征学习
  - Automatic feature generation, e.g. convolutional neural network (CNN)

### [](#header-5)五、高斯密度下的判别函数

### [](#header-6)六、离散变量的贝叶斯决策

离散特征变量，概率密度函数$p(\mathbf{x} \mid \omega_i) = p(x_1x_2 \cdots x_d \mid \omega_i)$

### [](#header-7)七、复合模式分类

### [](#header-8)八、概率密度估计方法

该文章已同步至[CSDN](http://blog.csdn.net/u013413471/article/)  
